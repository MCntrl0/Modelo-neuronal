# -*- coding: utf-8 -*-
"""GPU_vs_CPU_CNN_cifar100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rvs7lX1dXmc2YX8dpIm3Y_Eu_B7SyFZx

# Ejemplo de CNN 2D en la base de datos cifar100
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals
# Installa TensorFlow
# %tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)

# descargamos los datos cifar100 y los partimos en train y test
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()

# Tamaño de los datos (#datos,#lado,#alto,#canales)
print(X_train.shape)

import matplotlib.pyplot as plt

# enseñamos la primera imagen del dataset
plt.imshow(X_train[0],cmap="gray")

# one-hot encoding en las 'y'
from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

y_train.shape

plt.plot(y_train[0,:],'.-')

"""Modelo"""

# Hacemos un modelo con varias capas

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Dropout
from tensorflow.keras import regularizers

#crear modelo
model = Sequential()

#Vamos añadiendo capas
model.add(Conv2D(70, kernel_size=3, activation='relu', input_shape=(32,32,3),
                activity_regularizer=regularizers.l1(0.000001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(35, kernel_size=3, activation='relu',
                activity_regularizer=regularizers.l1(0.000001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(27, kernel_size=3, activation='relu',
                activity_regularizer=regularizers.l1(0.000001)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(100, activation='softmax'))

# Compilamos el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Este es el modelo
model.summary()

# Vamos a usar el callback de earlystopping
from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=15, min_delta=0.01)

# Entrenamos
hist = model.fit(X_train, y_train, validation_split=0.3, epochs=20, callbacks=[early_stopping])

# Evaluamos el modelo
score = model.evaluate(X_test,y_test)
score

# Commented out IPython magic to ensure Python compatibility.
# Ploteamos la evolucion del loss durante el entrenamiento
import matplotlib.pyplot as plt

# %matplotlib inline

plt.figure
plt.plot(hist.history['loss'],label="loss")
plt.plot(hist.history['val_loss'],label="val_loss")
plt.legend()

# Commented out IPython magic to ensure Python compatibility.
# Ploteamos la evolucion del accuracy durante el entrenamiento
import matplotlib.pyplot as plt

# %matplotlib inline

plt.figure
plt.plot(hist.history['accuracy'],label="accuracy")
plt.plot(hist.history['val_accuracy'],label="val_accuracy")
plt.legend()

# Predecimos sobre test
preds_test = model.predict(X_test)
preds_test.shape

import numpy as np
# Elegimos una imagen de test
idx_mostrar = 2990
# Mostramos la imagen
plt.imshow(X_test[idx_mostrar,:,:,:])
# Mostramos las probabilidades que da el modelo a cada clase
print(np.argmax(preds_test[idx_mostrar]))